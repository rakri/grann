{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScaNN Demo with GloVe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import time\n",
    "import struct\n",
    "import itertools\n",
    "\n",
    "import scann\n",
    "\n",
    "from core_utils import read_float_binary_file,read_i8_binary_file,read_u8_binary_file,write_float_binary_file,read_int32_binary_file,compute_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 200), (50, 300), (50, 400), (50, 500), (100, 200), (100, 300), (100, 400), (100, 500), (150, 200), (150, 300), (150, 400), (150, 500), (200, 200), (200, 300), (200, 400), (200, 500), (250, 200), (250, 300), (250, 400), (250, 500)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "[nb, nd, dataset] = read_float_binary_file('/home/rakri/arxiv/wikipedia_large/wikipedia_base_100K.bin')\n",
    "[nq, ndq, queries] = read_float_binary_file('/home/rakri/arxiv/wikipedia_large/wikipedia_query.bin')\n",
    "[nqgt, nkgt, gt] = read_int32_binary_file('/home/rakri/wiki100k_gs100.bin')\n",
    "\n",
    "#names and parameters\n",
    "dataset_name='wiki100'\n",
    "L = 2000\n",
    "qtau = 0.2\n",
    "index_name = '/home/rakri/indices/scann_wiki_'+dataset_name+'l='+str(L)+'_qtau='+str(qtau) \n",
    "\n",
    "#for scann, search params are num leaves and num reorder\n",
    "\n",
    "leaf_list = [x for x in range(50, 251, 50)]\n",
    "reorder_list = [x for x in range(200, 501, 100)]\n",
    "\n",
    "#search_params = [(lc, rc) for lc, rc in zip(leaf_list, reorder_list)]\n",
    "search_params = list(itertools.product(leaf_list, reorder_list))\n",
    "\n",
    "print (search_params)\n",
    "#search_params = [(10, 100), (20, 100), (30, 100), (40), (50), (60), (70), (80), (90), (100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ScaNN searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index exists, so loading\n"
     ]
    }
   ],
   "source": [
    "#normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]\n",
    "# configure ScaNN as a tree - asymmetric hash hybrid with reordering\n",
    "# anisotropic quantization as described in the paper; see README\n",
    "\n",
    "# use scann.scann_ops.build() to instead create a TensorFlow-compatible searcher\n",
    "#searcher = scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\").tree(\n",
    "#    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
    "#    2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "\n",
    "if not os.path.exists(index_name):\n",
    "    os.makedirs(index_name)\n",
    "    searcher = scann.scann_ops_pybind.builder(dataset, 10, \"squared_l2\").tree(\n",
    "        num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
    "            2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "    searcher.serialize(index_name)\n",
    "else:\n",
    "    print(\"Index exists, so loading\")\n",
    "    searcher = scann.scann_ops_pybind.load_searcher(index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaNN interface features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves\treorder\tRecall\t\ttime\n",
      "50 \t 200 \t 0.2209 \t 459.31763648986816\n",
      "50 \t 300 \t 0.2209 \t 471.69079780578613\n",
      "50 \t 400 \t 0.2209 \t 491.87893867492676\n",
      "50 \t 500 \t 0.2209 \t 516.4154529571533\n",
      "100 \t 200 \t 0.34906 \t 519.577693939209\n",
      "100 \t 300 \t 0.34912 \t 546.4325428009033\n",
      "100 \t 400 \t 0.34912 \t 577.4599075317383\n",
      "100 \t 500 \t 0.34912 \t 596.8708038330078\n",
      "150 \t 200 \t 0.44786 \t 594.1460132598877\n",
      "150 \t 300 \t 0.44808 \t 618.553352355957\n",
      "150 \t 400 \t 0.44808 \t 648.908805847168\n",
      "150 \t 500 \t 0.44808 \t 682.8195571899414\n",
      "200 \t 200 \t 0.4527 \t 664.1870498657227\n",
      "200 \t 300 \t 0.45328 \t 692.0827388763428\n",
      "200 \t 400 \t 0.45328 \t 721.359395980835\n",
      "200 \t 500 \t 0.45328 \t 753.5184383392334\n",
      "250 \t 200 \t 0.47038 \t 738.4162425994873\n",
      "250 \t 300 \t 0.47112 \t 772.3684310913086\n",
      "250 \t 400 \t 0.47114 \t 807.6549530029297\n",
      "250 \t 500 \t 0.47114 \t 832.7040195465088\n"
     ]
    }
   ],
   "source": [
    "print(\"leaves\\treorder\\tRecall\\t\\ttime\")\n",
    "nk = 10\n",
    "for param in search_params:\n",
    "    (lc, rc) = param\n",
    "    neighbors = np.zeros((nq,nk))\n",
    "    start = time.time()\n",
    "    for i in range(0,nq):\n",
    "        neighbors[i,:], distances = searcher.search(queries[i], final_num_neighbors=nk,pre_reorder_num_neighbors=rc, leaves_to_search=lc)\n",
    "    end = time.time()\n",
    "    recall = compute_recall(neighbors, gt[:,:10])\n",
    "    print(lc,\"\\t\",rc,\"\\t\",recall,\"\\t\", 1000000*(end-start)/nq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.46818\n",
      "Time: 2.6787519454956055\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# sample codes from google repo\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    response = requests.get(\"http://ann-benchmarks.com/glove-100-angular.hdf5\")\n",
    "    loc = os.path.join(tmp, \"glove.hdf5\")\n",
    "    with open(loc, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    glove_h5py = h5py.File(loc, \"r\")\n",
    "list(glove_h5py.keys())    \n",
    "\n",
    "#dataset = glove_h5py['train']\n",
    "#queries = glove_h5py['test']\n",
    "#gt = glove_h5py['neighbors']\n",
    "\n",
    "# this will search the top 100 of the 2000 leaves, and compute\n",
    "# the exact dot products of the top 100 candidates from asymmetric\n",
    "# hashing to get the final top 10 candidates.\n",
    "start = time.time()\n",
    "neighbors, distances = searcher.search_batched(queries)\n",
    "end = time.time()\n",
    "# we are given top 100 neighbors in the ground truth, so select top 10\n",
    "print(\"Recall:\", compute_recall(neighbors, gt[:, :10]))\n",
    "print(\"Latency:\", (1000000*(end - start))/nq)\n",
    "\n",
    "# increasing the leaves to search increases recall at the cost of speed\n",
    "start = time.time()\n",
    "neighbors, distances = searcher.search_batched(queries, leaves_to_search=250)\n",
    "end = time.time()\n",
    "\n",
    "#print(\"Recall:\", compute_recall(neighbors, gt[:, :10]))\n",
    "print(\"Recall:\", compute_recall(neighbors, gt[:, :10]))\n",
    "print(\"Latency:\", (1000000*(end - start))/nq)\n",
    "\n",
    "# increasing reordering (the exact scoring of top AH candidates) has a similar effect.\n",
    "start = time.time()\n",
    "neighbors, distances = searcher.search_batched(queries, leaves_to_search=250, pre_reorder_num_neighbors=500)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Recall:\", compute_recall(neighbors, gt[:, :10]))\n",
    "print(\"Latency:\", (1000000*(end - start))/nq)\n",
    "\n",
    "# we can also dynamically configure the number of neighbors returned\n",
    "# currently returns 10 as configued in ScannBuilder()\n",
    "neighbors, distances = searcher.search_batched(queries)\n",
    "print(neighbors.shape, distances.shape)\n",
    "\n",
    "# now returns 20\n",
    "neighbors, distances = searcher.search_batched(queries, final_num_neighbors=20)\n",
    "print(neighbors.shape, distances.shape)\n",
    "\n",
    "# we have been exclusively calling batch search so far; the single-query call has the same API\n",
    "start = time.time()\n",
    "for i in range(0,nq):\n",
    "    neighbors, distances = searcher.search(queries[i], final_num_neighbors=10, leaves_to_search=200, pre_reorder_num_neighbors = 250)\n",
    "end = time.time()\n",
    "\n",
    "print(neighbors)\n",
    "print(distances)\n",
    "print(\"Latency (ms):\", 1000000*(end - start)/nq)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
